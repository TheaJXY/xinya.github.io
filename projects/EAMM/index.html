<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model." />
	<meta property="og:description" content="Webpage of EAMM algorithm." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">EAMM: One-Shot Emotional Talking Face 
			via Audio-Based <br>
		Emotion-Aware Motion Model</span>
		<table align=center width=600px>
			<br> </br>
			<table align=center width=600px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:18px">Xinya Ji</span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:18px"><a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:18px">Kaisiyuan Wang</span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:18px"><a href="https://wuqianyi.top/">Qianyi Wu</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=600px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:18px"><a href="http://wywu.github.io/">Wayne Wu</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:18px"><a href="http://xufeng.site/">Feng Xu</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:18px"><a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=700px>
				<tr>
					<td align=center width=170px>
						<center>
							<span style="font-size:16px">Nanjing University</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:16px">The Chinese University of Hong Kong</span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:16px">The University of Sydney</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=700px>
				<tr>
					<td align=center width=170px>
						<center>
							<span style="font-size:16px">Monash University</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:16px">SenseTime Research</span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:16px">Tsinghua University</span>
						</center>
					</td>
				</tr>
			</table>
			&nbsp;
			<table align=center width=400px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px">SIGGRAPH Conference Proceedings 2022</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:18px"><a href='./resources/EAMM_SIG2022.pdf'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:18px"><a href='./resources/EAMM_SIG2022_sup.pdf'>[Supp]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:18px"><a href='https://github.com/jixinya/EAMM/'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:750px" src="./resources/teaser1.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					<span style="font-size:15px">Given a single portrait image, we can synthesize emotional
talking faces, where mouth movements match the input audio and facial emotion dynamics follow the emotion source video</span>.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=900px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Although significant progress has been made to audio-driven talking face generation, existing methods either neglect facial emotion or cannot be applied to arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model (EAMM) to generate oneshot emotional talking faces by involving an emotion source video. Specifically, we first propose an Audio2Facial-Dynamics module, which renders talking faces from audio-driven unsupervised zeroand first-order key-points motion. Then through exploring the motion modelâ€™s properties, we further propose an Implicit Emotion Displacement Learner to represent emotion-related facial dynamics as linearly additive displacements to the previously acquired motion representations. Comprehensive experiments demonstrate that by incorporating the results from both modules, our method can generate satisfactory talking face results on arbitrary subjects with realistic emotion patterns.
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Video</h1></center>
	
	<p align="center">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/VT53MzPzhWM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		<!-- Video player 1422x800 
		<video width="660" height="395" controls autoplay>
		  <source src="./resources/00822-video.mp4" type="video/mp4">							  
		Your browser does not support the video tag.
		</video>-->
	</p>


	<hr>

	<center><h1>Code</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/pipeline1.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<center>
					Overview of our Emotion-Aware Motion Model.
				</center>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:20px">&nbsp;<a href='hhttps://github.com/jixinya/EAMM/'>[GitHub]</a>
			</center>
		</span>
	</table>
	<br>
	<hr>
	<table align=center width=650px>
		<center><h1>Paper</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/EAMM_1.png"/></a></td>
			<td><span style="font-size:14pt"><b>EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model.</b><br>
				In SIGGRAPH Conference Proceedings, 2022.<br>
				(hosted on <a href="https://arxiv.org/pdf/2205.15278.pdf">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:15pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					TThis work was supported by the NSFC (No.62025108, 62021002, 61727808), the NSFJS (BK20192003), the Beijing Natural Science Foundation (JQ19015), the National Key R&D Program of China 2018YFA0704000. This work was supported by the Institute for Brain and Cognitive Science, Tsinghua University (THUIBCS) and Beijing Laboratory of Brain and Cognitive Intelligence, Beijing Municipal Education Commission (BLBCI).
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>
